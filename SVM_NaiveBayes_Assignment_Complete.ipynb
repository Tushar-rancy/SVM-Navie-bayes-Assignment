{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tushar-rancy/SVM-Navie-bayes-Assignment/blob/main/SVM_NaiveBayes_Assignment_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca7901a",
      "metadata": {
        "id": "6ca7901a"
      },
      "source": [
        "# SVM & Naive Bayes – Assignment Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87f77e0",
      "metadata": {
        "id": "c87f77e0"
      },
      "source": [
        "### Q1. What is a Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7001a1e0",
      "metadata": {
        "id": "7001a1e0"
      },
      "source": [
        "**Answer:**\n",
        "A Support Vector Machine is a supervised learning model used for classification and regression tasks. It finds the optimal hyperplane that maximally separates classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425a9460",
      "metadata": {
        "id": "425a9460"
      },
      "source": [
        "### Q2. What is the difference between Hard Margin and Soft Margin SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d968888b",
      "metadata": {
        "id": "d968888b"
      },
      "source": [
        "**Answer:**\n",
        "Hard Margin SVM strictly separates classes with no tolerance for misclassification. Soft Margin SVM allows some errors for better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3368cfa",
      "metadata": {
        "id": "c3368cfa"
      },
      "source": [
        "### Q3. What is the mathematical intuition behind SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ea3f27",
      "metadata": {
        "id": "e1ea3f27"
      },
      "source": [
        "**Answer:**\n",
        "SVM aims to maximize the margin between support vectors of different classes while minimizing classification errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02e8437",
      "metadata": {
        "id": "e02e8437"
      },
      "source": [
        "### Q4. What is the role of Lagrange Multipliers in SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ae0c77",
      "metadata": {
        "id": "69ae0c77"
      },
      "source": [
        "**Answer:**\n",
        "They help solve the constrained optimization problem in SVM by transforming it into a dual problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b5bc09",
      "metadata": {
        "id": "25b5bc09"
      },
      "source": [
        "### Q5. What are Support Vectors in SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "740ff660",
      "metadata": {
        "id": "740ff660"
      },
      "source": [
        "**Answer:**\n",
        "Support vectors are data points closest to the decision boundary; they influence the position and orientation of the hyperplane."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe873a5a",
      "metadata": {
        "id": "fe873a5a"
      },
      "source": [
        "### Q6. What is a Support Vector Classifier (SVC)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d2e679",
      "metadata": {
        "id": "52d2e679"
      },
      "source": [
        "**Answer:**\n",
        "SVC is an SVM used for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3d18cf",
      "metadata": {
        "id": "2c3d18cf"
      },
      "source": [
        "### Q7. What is a Support Vector Regressor (SVR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0659507",
      "metadata": {
        "id": "d0659507"
      },
      "source": [
        "**Answer:**\n",
        "SVR is an SVM variant used for regression that fits a margin of tolerance (epsilon) around the predicted function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c584e9bb",
      "metadata": {
        "id": "c584e9bb"
      },
      "source": [
        "### Q8. What is the Kernel Trick in SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc812cfd",
      "metadata": {
        "id": "bc812cfd"
      },
      "source": [
        "**Answer:**\n",
        "The kernel trick maps data into higher dimensions to make it linearly separable without explicitly computing the transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10325126",
      "metadata": {
        "id": "10325126"
      },
      "source": [
        "### Q9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "493cb289",
      "metadata": {
        "id": "493cb289"
      },
      "source": [
        "**Answer:**\n",
        "- Linear: best for linearly separable data\n",
        "- Polynomial: maps input features to a higher-degree polynomial space\n",
        "- RBF: captures complex relationships by using distance-based similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca509ed7",
      "metadata": {
        "id": "ca509ed7"
      },
      "source": [
        "### Q10. What is the effect of the C parameter in SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642e7166",
      "metadata": {
        "id": "642e7166"
      },
      "source": [
        "**Answer:**\n",
        "C controls the trade-off between maximizing the margin and minimizing classification errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0b3cae",
      "metadata": {
        "id": "6d0b3cae"
      },
      "source": [
        "### Q11. What is the role of the Gamma parameter in RBF Kernel SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c98c512",
      "metadata": {
        "id": "4c98c512"
      },
      "source": [
        "**Answer:**\n",
        "Gamma defines the influence of a single training example. Low values mean far influence; high values mean close influence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9760d88c",
      "metadata": {
        "id": "9760d88c"
      },
      "source": [
        "### Q12. What is the Naïve Bayes classifier, and why is it called 'Naïve'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03845f10",
      "metadata": {
        "id": "03845f10"
      },
      "source": [
        "**Answer:**\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ theorem with the assumption that features are conditionally independent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "071dab77",
      "metadata": {
        "id": "071dab77"
      },
      "source": [
        "### Q13. What is Bayes’ Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b2f2c1",
      "metadata": {
        "id": "18b2f2c1"
      },
      "source": [
        "**Answer:**\n",
        "P(A|B) = (P(B|A) * P(A)) / P(B)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d8d391",
      "metadata": {
        "id": "60d8d391"
      },
      "source": [
        "### Q14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85714e5d",
      "metadata": {
        "id": "85714e5d"
      },
      "source": [
        "**Answer:**\n",
        "- Gaussian: for continuous data\n",
        "- Multinomial: for count data\n",
        "- Bernoulli: for binary features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e70a3a90",
      "metadata": {
        "id": "e70a3a90"
      },
      "source": [
        "### Q15. When should you use Gaussian Naïve Bayes over other variants"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a92f53",
      "metadata": {
        "id": "17a92f53"
      },
      "source": [
        "**Answer:**\n",
        "When the features are continuous and normally distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aacadc1",
      "metadata": {
        "id": "6aacadc1"
      },
      "source": [
        "### Q16. What are the key assumptions made by Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79de9875",
      "metadata": {
        "id": "79de9875"
      },
      "source": [
        "**Answer:**\n",
        "Features are conditionally independent given the class label."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efecb16d",
      "metadata": {
        "id": "efecb16d"
      },
      "source": [
        "### Q17. What are the advantages and disadvantages of Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff051b2",
      "metadata": {
        "id": "9ff051b2"
      },
      "source": [
        "**Answer:**\n",
        "**Advantages**: Simple, fast, effective for text. **Disadvantages**: Strong independence assumption."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e33174b",
      "metadata": {
        "id": "8e33174b"
      },
      "source": [
        "### Q18. Why is Naïve Bayes a good choice for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d4e33a",
      "metadata": {
        "id": "60d4e33a"
      },
      "source": [
        "**Answer:**\n",
        "Text data often satisfies the conditional independence assumption and Naïve Bayes performs well with sparse data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bfabb4a",
      "metadata": {
        "id": "2bfabb4a"
      },
      "source": [
        "### Q19. Compare SVM and Naïve Bayes for classification tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "530fd696",
      "metadata": {
        "id": "530fd696"
      },
      "source": [
        "**Answer:**\n",
        "SVM is more flexible and accurate for complex datasets; Naïve Bayes is faster and suitable for text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4138bc",
      "metadata": {
        "id": "fc4138bc"
      },
      "source": [
        "### Q20. How does Laplace Smoothing help in Naïve Bayes?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82ff420c",
      "metadata": {
        "id": "82ff420c"
      },
      "source": [
        "**Answer:**\n",
        "It avoids zero probability by adding a small constant to each count."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "171cf78b",
      "metadata": {
        "id": "171cf78b"
      },
      "source": [
        "## Practical Implementation of SVM & Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1f30a4",
      "metadata": {
        "id": "fe1f30a4"
      },
      "source": [
        "### Q21. Import libraries and load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56b589f",
      "metadata": {
        "id": "e56b589f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv(\"your_dataset.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055dda23",
      "metadata": {
        "id": "055dda23"
      },
      "source": [
        "### Q22. Explore dataset structure and check for null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49e8fd6",
      "metadata": {
        "id": "f49e8fd6"
      },
      "outputs": [],
      "source": [
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd64022",
      "metadata": {
        "id": "1fd64022"
      },
      "source": [
        "### Q23. Split the data into features and target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57344316",
      "metadata": {
        "id": "57344316"
      },
      "outputs": [],
      "source": [
        "X = df.drop('target', axis=1)\n",
        "y = df['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab993a8",
      "metadata": {
        "id": "5ab993a8"
      },
      "source": [
        "### Q24. Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ee20d4",
      "metadata": {
        "id": "32ee20d4"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227e15da",
      "metadata": {
        "id": "227e15da"
      },
      "source": [
        "### Q25. Train an SVM with a linear kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95625b95",
      "metadata": {
        "id": "95625b95"
      },
      "outputs": [],
      "source": [
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72771ce",
      "metadata": {
        "id": "c72771ce"
      },
      "source": [
        "### Q26. Evaluate SVM with linear kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e46e723",
      "metadata": {
        "id": "8e46e723"
      },
      "outputs": [],
      "source": [
        "y_pred = svm_linear.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c89e275",
      "metadata": {
        "id": "3c89e275"
      },
      "source": [
        "### Q27. Train an SVM with RBF kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf1d185",
      "metadata": {
        "id": "8bf1d185"
      },
      "outputs": [],
      "source": [
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf6eec2",
      "metadata": {
        "id": "faf6eec2"
      },
      "source": [
        "### Q28. Train an SVM with polynomial kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033f88b4",
      "metadata": {
        "id": "033f88b4"
      },
      "outputs": [],
      "source": [
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea77aed",
      "metadata": {
        "id": "fea77aed"
      },
      "source": [
        "### Q29. Compare accuracies of SVM models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b1b985",
      "metadata": {
        "id": "75b1b985"
      },
      "outputs": [],
      "source": [
        "print('Linear:', accuracy_score(y_test, svm_linear.predict(X_test)))\n",
        "print('RBF:', accuracy_score(y_test, svm_rbf.predict(X_test)))\n",
        "print('Poly:', accuracy_score(y_test, svm_poly.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2779a7dd",
      "metadata": {
        "id": "2779a7dd"
      },
      "source": [
        "### Q30. Train a Gaussian Naïve Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba88cedd",
      "metadata": {
        "id": "ba88cedd"
      },
      "outputs": [],
      "source": [
        "nb_gauss = GaussianNB()\n",
        "nb_gauss.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e77cec1",
      "metadata": {
        "id": "6e77cec1"
      },
      "source": [
        "### Q31. Evaluate Gaussian Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e97dbb",
      "metadata": {
        "id": "c2e97dbb"
      },
      "outputs": [],
      "source": [
        "y_pred_nb = nb_gauss.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred_nb))\n",
        "print(classification_report(y_test, y_pred_nb))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34c0198",
      "metadata": {
        "id": "a34c0198"
      },
      "source": [
        "### Q32. Train a Multinomial Naïve Bayes model (for text/count data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b9439c",
      "metadata": {
        "id": "57b9439c"
      },
      "outputs": [],
      "source": [
        "# Only use if features are non-negative counts\n",
        "nb_multi = MultinomialNB()\n",
        "nb_multi.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbeb3bbc",
      "metadata": {
        "id": "cbeb3bbc"
      },
      "source": [
        "### Q33. Train a Bernoulli Naïve Bayes model (for binary features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aacb9eb5",
      "metadata": {
        "id": "aacb9eb5"
      },
      "outputs": [],
      "source": [
        "nb_bern = BernoulliNB()\n",
        "nb_bern.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1536ce77",
      "metadata": {
        "id": "1536ce77"
      },
      "source": [
        "### Q34. Plot confusion matrix for best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5e3304",
      "metadata": {
        "id": "9e5e3304"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96afebe8",
      "metadata": {
        "id": "96afebe8"
      },
      "source": [
        "### Q35. Compare performance of SVM and Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d501fb29",
      "metadata": {
        "id": "d501fb29"
      },
      "outputs": [],
      "source": [
        "print('SVM Accuracy:', accuracy_score(y_test, svm_rbf.predict(X_test)))\n",
        "print('NB Accuracy:', accuracy_score(y_test, nb_gauss.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7226cf6f",
      "metadata": {
        "id": "7226cf6f"
      },
      "source": [
        "### Q36. Apply feature scaling and retrain SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baae6a33",
      "metadata": {
        "id": "baae6a33"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "svm_scaled = SVC(kernel='rbf')\n",
        "svm_scaled.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e4dd87",
      "metadata": {
        "id": "f7e4dd87"
      },
      "source": [
        "### Q37. Evaluate scaled SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "718e18a1",
      "metadata": {
        "id": "718e18a1"
      },
      "outputs": [],
      "source": [
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred_scaled))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b57156",
      "metadata": {
        "id": "36b57156"
      },
      "source": [
        "### Q38. Use GridSearchCV to find best SVM parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69dc0f33",
      "metadata": {
        "id": "69dc0f33"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf']}\n",
        "grid = GridSearchCV(SVC(), param_grid=params, refit=True, verbose=0)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "print('Best Params:', grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c51a4f2",
      "metadata": {
        "id": "8c51a4f2"
      },
      "source": [
        "### Q39. Evaluate best model from GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af93ab5b",
      "metadata": {
        "id": "af93ab5b"
      },
      "outputs": [],
      "source": [
        "best_model = grid.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test_scaled)\n",
        "print(accuracy_score(y_test, y_pred_best))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6aaf17d",
      "metadata": {
        "id": "f6aaf17d"
      },
      "source": [
        "### Q40. Train an SVR model for regression task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b8abf6",
      "metadata": {
        "id": "a3b8abf6"
      },
      "outputs": [],
      "source": [
        "svr_model = SVR(kernel='rbf')\n",
        "svr_model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2daebc",
      "metadata": {
        "id": "1f2daebc"
      },
      "source": [
        "### Q41. Evaluate SVR model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0f9a0f",
      "metadata": {
        "id": "2b0f9a0f"
      },
      "outputs": [],
      "source": [
        "print('SVR Score:', svr_model.score(X_test_scaled, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df47f38d",
      "metadata": {
        "id": "df47f38d"
      },
      "source": [
        "### Q42. Plot SVM decision boundary (2D example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e0ffb3",
      "metadata": {
        "id": "35e0ffb3"
      },
      "outputs": [],
      "source": [
        "# Only works for 2-feature data\n",
        "import numpy as np\n",
        "h = .02\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = best_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "906d4786",
      "metadata": {
        "id": "906d4786"
      },
      "source": [
        "### Q43. Save trained SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15967369",
      "metadata": {
        "id": "15967369"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(best_model, 'svm_best_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159d8a96",
      "metadata": {
        "id": "159d8a96"
      },
      "source": [
        "### Q44. Load and test saved SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d953ab",
      "metadata": {
        "id": "63d953ab"
      },
      "outputs": [],
      "source": [
        "loaded_model = joblib.load('svm_best_model.pkl')\n",
        "print('Loaded model score:', loaded_model.score(X_test_scaled, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9039276e",
      "metadata": {
        "id": "9039276e"
      },
      "source": [
        "### Q45. Save trained Naïve Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d628457",
      "metadata": {
        "id": "7d628457"
      },
      "outputs": [],
      "source": [
        "joblib.dump(nb_gauss, 'naive_bayes_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132faaad",
      "metadata": {
        "id": "132faaad"
      },
      "source": [
        "### Q46. Load and test saved Naïve Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5e37dc",
      "metadata": {
        "id": "9a5e37dc"
      },
      "outputs": [],
      "source": [
        "loaded_nb = joblib.load('naive_bayes_model.pkl')\n",
        "print('Loaded NB model score:', loaded_nb.score(X_test, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}